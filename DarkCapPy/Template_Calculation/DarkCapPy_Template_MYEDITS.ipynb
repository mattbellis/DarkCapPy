{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DarkCapPy Template\n",
    "\n",
    "Author: Adam Green\n",
    "\n",
    "Email : agree019@ucr.edu\n",
    "\n",
    "Date: 8/7/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This package is set up to perform \"out-of-the-box\" calculations. Some calculations, in particular the reach plot for IceCube, may take hours of run time. If the time intensive calculations are interrupted, they must be restarted from the beginning. This notebook is a guide to using the package to perform time-intensive parameter scans. This package is optimized to perform parameter scans over mediator mass $m_{A'}$ and kinetic mixing $\\varepsilon$ for a fixed dark matter mass $m_X$.\n",
    "\n",
    "This notebook utilizes [pandas](https://pandas.pydata.org/) to read and write intermediate results to external comma-separated-value (csv) files. These files allow longer calculations to be interrupted and resumed, even if the Jupyter kernel is restarted.\n",
    "\n",
    "This notebook generates 2 csv files, which we refer to as $\\texttt{Sommerfeld.csv}$ and $\\texttt{Signal.csv}$. It requires input from a third csv file which we refer to as $\\texttt{Branch.csv}$.\n",
    "\n",
    "This notebook is divided into four main sections titled Package Test, Sommerfeld, Equilibrium Plots, and IceCube Signal. We provide a brief overview of each section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Package Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This section tests the package installation by calculating the number of signal events expected at IceCube for a fixxed set of parameters - dark matter mass, mediator mass, mediator coupling, and IceCube observation time. It outputs all intermediate results. This section fully demonstrates how to use DarkCapPy for calculations of the \"dark Earthshine\" scenario of a single set of parameters.\n",
    "\n",
    "_Estimated Run Time_: 4 Minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Sommerfeld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Sommerfeld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This section creates and populates $\\texttt{Sommerfeld.csv}$ with \n",
    "\n",
    "1. A single value of the dark matter mass $m_X$ in GeV, \n",
    "2. A list of mediator masses $m_{A'}$ in GeV\n",
    "3. A list of corresponding thermally averaged Sommerfeld enhancements $\\langle S_S(m_{A'}) \\rangle$ \n",
    "4. The part of the capture rate which only depends on $m_X$ and $\\alpha_X$ $\\kappa_0$ in units of GeV$^5$, $\\kappa_0$.\n",
    "\n",
    "The user is prompted to input a value of $m_X$ when $\\texttt{Sommerfeld.csv}$ is created. A list of mediator masses, $m_{A'}$ , are used to generate corresponding values of the thermally averaged Sommerfeld effect, $\\langle S_S \\rangle$. The quantity $\\kappa_0$ depends only on $m_X$ and $\\alpha_X$. When $m_X$ is fixed, $\\kappa_0$ is uniquely determined by fixing $\\alpha_X$ to give the correct dark matter abundance from thermal freeze out. The $\\texttt{Sommerfeld.csv}$ file generated from this section is used as input for all later parts of this notebook.\n",
    "\n",
    "_Estimated Run Time_: 6 Minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Equilibrium Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This section reads in a completed $\\texttt{Sommerfeld.csv}$ and generates plots of the equilibrium time in $(\\varepsilon, m_{A'})$ space. The equilibrium time $\\tau$ is the time it takes the captured dark matter population to reach a stable value. By default, we plot contour values of $\\tau/\\tau_\\oplus = \\{ 10^{-4} ,10^{-2}, 10^0, 10^2, 10^4 \\}$. Where $\\tau_\\oplus = 4.5$ GYr is the age of the Earth.\n",
    "\n",
    "_Estimated Run Time_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### IceCube Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This section creates and populates $\\texttt{Signal.csv}$ file that stores values pertaining to the number of signal events at IceCube. This section is the most time consuming and computation intensive section. This is because the typial resolution of these plots requires $\\mathcal{O}(10^4)$ calculations.\n",
    "\n",
    "This section reads in (1) $\\texttt{Sommerfeld.csv}$ and (2) $\\texttt{Branch.csv}$ and outputs $\\texttt{Signal.csv}$ which stores the range of mediator masses mass. \n",
    "\n",
    "When the calculations are complete, this section also reads in a completed $\\texttt{Signal.csv}$ file and plots order-of-magnitude contours of the number of signal events at IceCube $N_\\text{sig}$ against mediator mass $m_{A'}$ on the horizontal axis and kinetic mixing parameter $\\varepsilon$ on the vertical axis. By default, the contour values are $N_\\text{sig} = \\{ 1, 10, 100, 1000 \\}$.\n",
    "\n",
    "As a benchmark, the calculation rate on a modern laptop is about 140 points / minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we import the required libraries and define the paths to each data file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T19:53:21.363773Z",
     "start_time": "2019-06-22T19:50:38.710214Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interpolate\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "\n",
    "from DarkCapPy import *\n",
    "import DarkCapPy.DarkPhoton as DP\n",
    "\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "###################\n",
    "# Define File Paths\n",
    "###################\n",
    "# These paths are specific to the preset folders in Temp;ate_Caluclation\n",
    "def sommerfeldPath(file):\n",
    "    path = 'Sommerfeld/' + file\n",
    "    return path\n",
    "\n",
    "def branchPath(file):\n",
    "    path = 'Branching_Ratio/' + file\n",
    "    return path\n",
    "\n",
    "def signalPath(file):\n",
    "    path = 'Signal/' + file\n",
    "    return path\n",
    "    \n",
    "def signalBackupPath(file):\n",
    "    path = 'Signal/Signal_Backups/' + file\n",
    "    return path\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Test  (Single Parameter Point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is a self-contained test of `DarkCapPy`. It outputs the inermediate results and total number of singnal events for a set of model parameters.\n",
    "\n",
    "We initliaize our parameters to match the values chosen for Table 1 of [arXiv:1509.07525](https://arxiv.org/abs/1509.07525) and generate the corresponding $N_\\text{sig}$ point in $(m_{A'}, \\varepsilon)$ space, taking the effective area of IceCube to be $A_\\text{eff} = 1$ km$^2$ and the observation time to be $T = 10$ yr. All masses and energies are assumed to be in units of GeV. \n",
    "\n",
    "For Earth, this cell takes about 6 minutes to run.\n",
    "\n",
    "For a single element in the Sun, this cell takes about 6 minutes to run.\n",
    "\n",
    "Input parameters:\n",
    "\n",
    "    mx       = 1000\n",
    "    ma       = 1\n",
    "    epsilon  = 1e-8\n",
    "    alpha    = 1/137\n",
    "    alphax   = 0.035\n",
    "    tauCross = DP.tauCross = 1.41912e+17\n",
    "\n",
    "The anticipated output for Earth is:\n",
    "\n",
    "    Capture_1            : 109069092.29572794\n",
    "    Kappa_0              : 3.116357975166786e+25\n",
    "    Capture_2            : 109072529.13083751\n",
    "    Therm Avg Sommerfeld : 238.71863691039448\n",
    "    Sigma V              : 3.8484490764205524e-09\n",
    "    Annihilation         : 1.8311290915841913e-46\n",
    "    EQ Time              : 7.07603006302535e+18\n",
    "    Gamma_ann            : 21928.76449347939\n",
    "    Decay Length         : 82568160.0\n",
    "    Epsilon_decay        : 5.393699655806077e-07\n",
    "    N_signal             : 0.01462555822956747"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure we are using the correct versions of thing\n",
    "\n",
    "# The Earth is about 6000 km in radius\n",
    "\n",
    "print(f'Planet mass:   {DP.Planet_Mass} grams')\n",
    "print(f'Planet radius: {DP.Planet_Radius} cm')\n",
    "print(f'Planet radius: {DP.Planet_Radius/1e5} km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_k1 = pd.read_parquet('different_kappas_MANY_MX_MASSES.parquet')\n",
    "df_k2 = pd.read_parquet('different_kappas_TEST.parquet')\n",
    "df_k3 = pd.read_parquet('different_kappas_HIGH_MX_MASSES.parquet')\n",
    "\n",
    "print(df_k1.iloc[0:3])\n",
    "print(df_k2.iloc[0:3])\n",
    "\n",
    "print('------------------------')\n",
    "print(df_k1.iloc[28:31])\n",
    "print(df_k2.iloc[28:31])\n",
    "print(df_k3.iloc[0:3])\n",
    "\n",
    "print('------------------------')\n",
    "print(df_k1.iloc[-3:])\n",
    "print(df_k2.iloc[-3:])\n",
    "print(df_k3.iloc[-3:])\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T19:59:48.150700Z",
     "start_time": "2019-06-22T19:53:27.053573Z"
    }
   },
   "outputs": [],
   "source": [
    "mx = 1000000\n",
    "ma = 0.30\n",
    "epsilon = 1e-8\n",
    "alpha = 1/137\n",
    "#alphax = 0.035\n",
    "tauCross = DP.tauCross\n",
    "\n",
    "#start = time.time()\n",
    "#print(\"Starting cCap...\")\n",
    "#cap1 = DP.cCap(mx,ma,epsilon,alpha,alphax)\n",
    "#print(f\"Finished in {time.time() - start:.2f} seconds\")\n",
    "#print ('Capture_1            :', cap1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "#print(\"Starting kappa0...\")\n",
    "#kappa0 = DP.kappa_0(mx,alpha)\n",
    "#print(f\"Finished in {time.time() - start:.2f} seconds\")\n",
    "#print ('Kappa_0              :', kappa0)\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "# for testing\n",
    "#######################\n",
    "#'''\n",
    "#df_kappa = pd.read_parquet('different_kappas_03052025_many_mxs.parquet')\n",
    "#df_kappa = pd.read_parquet('different_kappas_MANY_MX_MASSES.parquet')\n",
    "df_kappa = pd.read_parquet('different_kappas_MX_1000_to_10000.parquet')\n",
    "\n",
    "\n",
    "mx = 1000\n",
    "filter = df_kappa['mx'] == mx\n",
    "kappa0 = df_kappa[filter]['kappa0'].values[0]\n",
    "print ('Kappa_0              :', kappa0)\n",
    "print()\n",
    "\n",
    "df_kappa\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(\"Starting cCapQuick...\")\n",
    "alphax = DP.alphaTherm(mx, ma)\n",
    "cap2 = DP.cCapQuick(mx,ma,epsilon,alphax,kappa0)\n",
    "print(f\"Finished in {time.time() - start:.2f} seconds\")\n",
    "print ('Capture_2            :', cap2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(\"Starting sommerfeld...\")\n",
    "\n",
    "sommerfeld = DP.thermAvgSommerfeld(mx,ma,alphax)\n",
    "\n",
    "print(f\"Finished in {time.time() - start:.2f} seconds\")\n",
    "print ('Therm Avg Sommerfeld :', sommerfeld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "print(\"Starting sigma...\")\n",
    "sigma = DP.sigmaVtree(mx,ma,alphax)\n",
    "print(f\"Finished in {time.time() - start:.2f} seconds\")\n",
    "print ('Sigma V              :', sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "print(\"Starting the others...\")\n",
    "\n",
    "ann = DP.cAnn(mx,sigma,sommerfeld)\n",
    "\n",
    "# Use cap2!!!!\n",
    "\n",
    "tau = DP.tau(cap2,ann)\n",
    "gammaAnn = DP.gammaAnn(cap2,ann)\n",
    "L = DP.decayLength(mx,ma,epsilon,1)\n",
    "Edecay = DP.epsilonDecay(L)\n",
    "\n",
    "signal = DP.iceCubeSignal(gammaAnn,Edecay,DP.yr2s(10))\n",
    "print(f\"Finished in {time.time() - start:.2f} seconds\")\n",
    "print()\n",
    "\n",
    "#print ('Capture_1            :', cap1)\n",
    "print ('Kappa_0              :', kappa0)\n",
    "print ('Capture_2            :', cap2)\n",
    "print ('Therm Avg Sommerfeld :', sommerfeld)\n",
    "print ('Sigma V              :', sigma)\n",
    "print ('Annihilation         :', ann)\n",
    "print ('EQ Time              :', tau)\n",
    "print ('Gamma_ann            :', gammaAnn)\n",
    "print ('Decay Length         :', L)\n",
    "print ('Epsilon_decay        :', Edecay)\n",
    "print ('N_signal             :', signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \n",
    "\n",
    "\n",
    "# \n",
    "# ann = DP.cAnn(mx,sigma,sommerfeld)\n",
    "\n",
    "# tau = DP.tau(cap1,ann)\n",
    "\n",
    "# gammaAnn = DP.gammaAnn(cap1,ann)\n",
    "# L = DP.decayLength(mx,ma,epsilon,1)\n",
    "# Edecay = DP.epsilonDecay(L)\n",
    "\n",
    "# signal = DP.iceCubeSignal(gammaAnn,Edecay,DP.yr2s(10))\n",
    "\n",
    "\n",
    "#print ('Capture_1            :', cap1)\n",
    "# print ('Kappa_0              :', kappa0)\n",
    "# print ('Capture_2            :', cap2)\n",
    "# print ('Therm Avg Sommerfeld :', sommerfeld)\n",
    "# print ('Sigma V              :', sigma)\n",
    "# print ('Annihilation         :', ann)\n",
    "# print ('EQ Time              :', tau)\n",
    "# print ('Gamma_ann            :', gammaAnn)\n",
    "# print ('Decay Length         :', L)\n",
    "# print ('Epsilon_decay        :', Edecay)\n",
    "# print ('N_signal             :', signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Sommerfeld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section creates $\\texttt{Sommerfeld.csv}$ that contains an user-defined number of $m_A$ and $\\langle S_S \\rangle$ values. \n",
    "\n",
    "$\\texttt{Sommerfeld.csv}$ has the following column headers:\n",
    "\n",
    "    Counter, mX[GeV], mA[GeV], ThermAvgSommerfeld, Kappa0[GeV5]\n",
    "\n",
    "- `Counter`: Stores the current row of the csv. This will be used as a loop index later.\n",
    "- `m_X[GeV]`: Dark matter mass in GeV specified at the prompt in [Create Sommerfeld.csv](#Create-Sommerfeld.csv).\n",
    "- `m_A[GeV]`: A list of numbers ranging from $0.01$ GeV to $10$ GeV with arbitrary spacing. Since these will each correspond to a single values of $\\langle S_S \\rangle$ to be interpolated later, we suggest using logarithmic spacing with a high density `num_Somm ~ 2500`.\n",
    "- `ThermAvgSommerfeld`: The corresponding Sommerfeld enhancement $\\langle S_S \\rangle$ for a given $m_{A'}$ value.\n",
    "- `Kappa0[GeV5]`: The part of the capture rate which only depends on $m_X$ and $\\alpha$ after the small recoil energy approximation. Once $m_X$ is fixed, this value is uniquely determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T16:12:05.967601Z",
     "start_time": "2018-04-03T16:12:05.961053Z"
    }
   },
   "source": [
    "## Define Sommerfeld Point Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the variable `num_Somm`, the resolution of the Sommerfeld enhancement. This number should be ~2500 because it is used to create an interpolation of the Sommerfeld enhancement $\\langle S_S \\rangle$. This interpolation, as opposed to the function `DarkCapPy.DarkPhoton.thermAvgSomm`, is called to calculate the number of signal events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:24:52.724407Z",
     "start_time": "2019-06-19T20:24:52.705457Z"
    }
   },
   "outputs": [],
   "source": [
    "num_Somm = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Paramter Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume the following range for $m_{A'}$:\n",
    "\n",
    "$$ 0.01 \\ \\text{GeV} \\leq m_{A'} \\leq 10 \\ \\text{GeV} $$\n",
    "\n",
    "This will use this range for the mediator mass for the rest of the calculations. The lower bound is fixed by requiring the dark photon to decay into electrons. The upper bound is adjusted to allow the dark photons to be relativistic when they are produced from dark matter annihilations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:24:54.186873Z",
     "start_time": "2019-06-19T20:24:54.164949Z"
    }
   },
   "outputs": [],
   "source": [
    "m_ALow = 10**-2\n",
    "m_AHigh= 10\n",
    "\n",
    "mALogRange = np.logspace(np.log10(m_ALow), np.log10(m_AHigh), num_Somm, base = 10)\n",
    "mAArray = []\n",
    "\n",
    "counter = 0\n",
    "for mATemp in mALogRange:\n",
    "                 #( Counter, m_X[GeV], m_A[Gev], ThermAvgSommerfeld, Kappa0[GeV5])\n",
    "    mAArray.append([counter, ' '     , mATemp  , 'None'            , ' '])\n",
    "    counter += 1\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sommerfeld.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filenames should follow a consistent convention since they will be used as input later on. A good convention for file names is:\n",
    "\n",
    "    <#><Unit>Sommerfeld.csv\n",
    "\n",
    "including the \".csv\" extension at the prompt.\n",
    "\n",
    "For example, if $m_X = 100$ GeV, a good filename would be something like:\n",
    "\n",
    "    100GeVSommerfeld.csv\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:25:03.759263Z",
     "start_time": "2019-06-19T20:24:57.515511Z"
    }
   },
   "outputs": [],
   "source": [
    "mX = 100 # Mass of DM candidate in GeV/c^2\n",
    "\n",
    "masterSommerfeldDataFrame = \\\n",
    "    pd.DataFrame(mAArray,\\\n",
    "                 columns = \\\n",
    "                 ['Counter','mX[GeV]', 'mA[GeV]', 'ThermAvgSommerfeld', 'Kappa0[GeV5]'])\n",
    "\n",
    "#sommFileName = input('Sommerfeld Filename: ')\n",
    "sommFileName = f\"{mX}GeVSommerfeld.csv\"\n",
    "\n",
    "if (sommFileName == ''):\n",
    "    print ('Setting default filename')\n",
    "    sommFileName = 'Sommerfeld_DEFAULT.csv'\n",
    "writeFile = sommerfeldPath(sommFileName)\n",
    "\n",
    "assert (sommFileName[-4:] == '.csv'), 'File must end with \".csv\"'\n",
    "\n",
    "\n",
    "masterSommerfeldDataFrame.to_csv(writeFile, index=False)\n",
    "dataFrame = pd.read_csv(writeFile , sep = ',')\n",
    "#m_XValue = float(input('m_X [GeV]: '))\n",
    "\n",
    "m_XValue = float(mX)\n",
    "\n",
    "kappa0Value = 'None'\n",
    "\n",
    "\n",
    "dataFrame.at[0, 'mX[GeV]'] = m_XValue\n",
    "dataFrame.at[0, 'Kappa0[GeV5]'] = kappa0Value\n",
    "\n",
    "dataFrame.to_csv(writeFile, index=False)\n",
    "\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating Sommerfeld.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell calculates:\n",
    "\n",
    "1. `Kappa0[GeV5]` = $\\kappa_0$, the part of the capture rate which is constant in $(m_{A'},\\varepsilon)$ space.\n",
    "\n",
    "2. `ThermAvgSommerfeld` = $\\langle S_S(m_{A'}) \\rangle$, the thermally-averaged Sommerfeld enhancement for each corresponding value of $m_A$.\n",
    "\n",
    "After each value of $\\langle S_S \\rangle$ is calculated, the $\\texttt{Sommerfeld.csv}$ is updated and overwritten. This cell should take about 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:30:25.780938Z",
     "start_time": "2019-06-19T20:25:06.225855Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sommFileName = input('Sommerfeld Filename: ')\n",
    "sommWriteFile = sommerfeldPath(sommFileName)\n",
    "sommDataIn = pd.read_csv(sommWriteFile,sep = ',')\n",
    "\n",
    "print (sommFileName)\n",
    "print (sommWriteFile)\n",
    "\n",
    "# Define the loop range\n",
    "looprange = len(sommDataIn['Counter'])\n",
    "\n",
    "# Read in m_X value\n",
    "Filem_X = float(sommDataIn.at[0,'mX[GeV]'])\n",
    "\n",
    "mX = 1000\n",
    "\n",
    "filter = df_kappa['mx'] == mX\n",
    "kappa0Value = df_kappa[filter]['kappa0'].values[0]\n",
    "\n",
    "\n",
    "\n",
    "#'''    \n",
    "##################################\n",
    "# Calculate Kappa0 and write it. \n",
    "##################################\n",
    "testKappa0 = sommDataIn.at[0,'Kappa0[GeV5]']\n",
    "\n",
    "print(f\"Read in {Filem_X}  {testKappa0}\")\n",
    "\n",
    "if (testKappa0 != testKappa0):\n",
    "    print ('Calculating Kappa0...')\n",
    "    \n",
    "    #kappa0Value = float(DP.kappa_0(Filem_X, 1./137))\n",
    "    kappa0Value = df_kappa[filter]['kappa0'].values[0]\n",
    "    \n",
    "    sommDataIn.at[0, 'Kappa0[GeV5]'] = kappa0Value\n",
    "    sommDataIn.to_csv(sommWriteFile, index=False)\n",
    "    print('Kappa0 value calculated and set')\n",
    "    \n",
    "elif (testKappa0 == testKappa0):\n",
    "    print ('Kappa0 value already calculated')\n",
    "#'''\n",
    "\n",
    "##################################\n",
    "# Calculate the Sommerfeld enhancements\n",
    "##################################\n",
    "\n",
    "finishedCounter = 0\n",
    "print ('-------------------')\n",
    "print ('Starting Sommerfeld Calculations...')\n",
    "for index in range(0,looprange):\n",
    "    ##################################\n",
    "    # Initialize Parameters\n",
    "    ##################################\n",
    "    m_AValue = sommDataIn.at[index,'mA[GeV]']\n",
    "    testSomm = sommDataIn.at[index,'ThermAvgSommerfeld']\n",
    "    \n",
    "    if (testSomm != testSomm):    \n",
    "        alpha_X = DP.alphaTherm(Filem_X, m_AValue)\n",
    "        thermAvgSomm = DP.thermAvgSommerfeld(Filem_X, m_AValue, alpha_X)\n",
    "\n",
    "        sommDataIn.at[index,'ThermAvgSommerfeld'] = thermAvgSomm\n",
    "        sommDataIn.to_csv(sommWriteFile, index=False)\n",
    "\n",
    "        finishedCounter +=1\n",
    "        if (index%20 == 0):\n",
    "            print ('Index: {0} recorded'.format(index))\n",
    "        \n",
    "    elif(testSomm == testSomm):\n",
    "        finishedCounter +=1\n",
    "        \n",
    "    if (finishedCounter == looprange):\n",
    "        print ('-------------------')\n",
    "        print ('All Calculations Complete')\n",
    "        \n",
    "    \n",
    "sommDataIn.to_csv(sommWriteFile, index=False)\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_kappas = pd.read_parquet('different_kappas.parquet')\n",
    "\n",
    "#df_kappas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should the previous cell take longer than is feasable, the caculation may be interrupted at any time by interrupting the Jupyter kernel (`esc I,I`). If the interruption occurs during the `to_csv` command, $\\texttt{Sommerfeld.csv}$ will be blank and it would appear that all the calculations have been lost. However, the dataframe is still stored in Jupyter memory. Manually writing this memory to $\\texttt{Sommerfeld.csv}$ will guarantee prevent this data from being lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:30:31.748666Z",
     "start_time": "2019-06-19T20:30:29.584735Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print ('Working Sommerfeld file: {0}'.format(sommWriteFile))\n",
    "\n",
    "overwrite = input('Overwrite? (y/n): ')\n",
    "if ((overwrite == 'y') or (overwrite == 'Y')):\n",
    "    sommDataIn.to_csv(sommWriteFile, index=False)\n",
    "    print ('Overwrite complete')\n",
    "    \n",
    "else:\n",
    "    print ('Overwrite aborted')\n",
    "\n",
    "\n",
    "print ('Complete')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Sommerfeld Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell:\n",
    "\n",
    "1. Reads in a completed $\\texttt{Sommerfeld.csv}$ file\n",
    "\n",
    "2. Plots the thermally-averaged Sommerfeld effect against mediator mass\n",
    "\n",
    "If $\\texttt{Sommerfeld.csv}$ isn't fully populated, i.e., the calculations have not been finished, Python will throw the following error:\n",
    "```python\n",
    "AttributeError: 'str' object has no attribute 'log10'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:30:39.753732Z",
     "start_time": "2019-06-19T20:30:35.258481Z"
    }
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "# Read in Sommerfeld CSV\n",
    "##################################\n",
    "#sommFile = input('Sommerfeld File: ')\n",
    "sommFile = sommFileName\n",
    "readFile = sommerfeldPath(sommFile)\n",
    "\n",
    "dataIn = pd.read_csv(readFile, sep = ',')\n",
    "\n",
    "\n",
    "#####################\n",
    "# Extract Plot Data\n",
    "#####################\n",
    "mAList = dataIn['mA[GeV]']\n",
    "SommerfeldList = dataIn['ThermAvgSommerfeld']\n",
    "Filem_X = float(dataIn.loc[0,'mX[GeV]'])\n",
    "\n",
    "##################################\n",
    "# Determine the diplay Units of m_X \n",
    "##################################\n",
    "m_XUnit = 'None'\n",
    "if (Filem_X < 1000):\n",
    "    m_XDisplay = Filem_X\n",
    "    m_XUnit = 'GeV'\n",
    "\n",
    "if (Filem_X >= 1000):\n",
    "    m_XDisplay = Filem_X*10**-3\n",
    "    m_XUnit = 'TeV'\n",
    "\n",
    "#####################\n",
    "# Plot\n",
    "#####################\n",
    "sommFig = plt.figure(figsize = (6,6))\n",
    "#Plot = plt.plot(np.log10(mAList),np.log10(SommerfeldList))\n",
    "plt.ylabel(r'$\\log( \\langle S_s \\rangle $', fontsize = 14)\n",
    "plt.xlabel(r'$\\log( m_A ) [GeV]$', fontsize = 14)\n",
    "\n",
    "Plot = plt.plot(mAList,SommerfeldList)\n",
    "plt.ylabel(r'$\\langle S_s \\rangle )$', fontsize = 14)\n",
    "plt.xlabel(r'$m_A [GeV]$', fontsize = 14)\n",
    "\n",
    "plt.suptitle('Sommerfeld Enhancement vs. $m_A$',fontsize = 16)\n",
    "plt.title(r'$m_X = {0}$ {1}'.format(m_XDisplay, m_XUnit),loc = 'right', fontsize = 14)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sommPlotFileName = input('Figure Name: ')\n",
    "sommPlotFileName = f\"{mx}GeV_Sommerfeld_vs_ma.png\"\n",
    "\n",
    "assert (sommPlotFileName != ''), 'No Filename'\n",
    "sommFig.savefig(sommPlotFileName, dpi = 700)\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Sommerfeld file interpolation\n",
    "\n",
    "from scipy import interpolate\n",
    "\n",
    "mx = 100\n",
    "mA = 1\n",
    "sommFileName = f'{mx}GeVSommerfeld.csv'\n",
    "sommFile = sommerfeldPath(sommFileName)\n",
    "dataIn = pd.read_csv(sommFile, sep = ',')\n",
    "\n",
    "###############################\n",
    "# Interpolate Sommerfeld\n",
    "###############################\n",
    "maList = dataIn['mA[GeV]']\n",
    "sommerfeldList = dataIn['ThermAvgSommerfeld']\n",
    "\n",
    "alphax = 0.035 * mx/1000\n",
    "alphax_max = 0.17 * (mx/1000)**1.61\n",
    "\n",
    "\n",
    "# Deprecated\n",
    "SommerfeldInterp = interpolate.interp1d(maList, sommerfeldList)\n",
    "\n",
    "Sommerfeld_interp = SommerfeldInterp(mA)\n",
    "sommerfeld = DP.thermAvgSommerfeld(mx,mA, alphax)\n",
    "\n",
    "\n",
    "alpha_X = DP.alphaTherm(mx, mA)\n",
    "alpha_X_approx = DP.alphaThermApprox(mx)\n",
    "\n",
    "thermAvgSomm = DP.thermAvgSommerfeld(mx, mA, alpha_X)\n",
    "thermAvgSomm_max = DP.thermAvgSommerfeld(mx, mA, alphax_max)\n",
    "\n",
    "\n",
    "print(\"alpha X\")\n",
    "print(alphax)\n",
    "print(alphax_max)\n",
    "\n",
    "print(alpha_X)\n",
    "print(alpha_X_approx)\n",
    "print()\n",
    "\n",
    "print(f\"alphax:         {alphax:8.6f}     sommerfeld: {sommerfeld:.4f}\" )\n",
    "print(f\"alpha_X (func): {alpha_X:8.6f}     sommerfeld: {thermAvgSomm:.4f}\" )\n",
    "print(f\"alpha_X (????): {0:8.6f}     sommerfeld: {Sommerfeld_interp:.4f}\" )\n",
    "print(f\"alpha_X (max) : {alphax_max:8.6f}     sommerfeld: {thermAvgSomm_max:.4f}\" )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My plots\n",
    "\n",
    "## Branching fraction decays\n",
    "\n",
    "https://en.wikipedia.org/wiki/Particle_decay\n",
    "\n",
    "For dark photons, we reference this paper.\n",
    "\n",
    "https://arxiv.org/pdf/1505.07459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to calculate them from phase space considerations\n",
    "\n",
    "# In the end, we will not use these and instead refer to the paper mentioned above. \n",
    "\n",
    "def calc_branching_fractions(MA, masses = []):\n",
    "\n",
    "    MA2 = MA**2 \n",
    "    \n",
    "    total = 0\n",
    "    terms = []\n",
    "    for mass in masses:\n",
    "        #term = np.sqrt(MA2 - 4*(mass**2))\n",
    "        #term = (np.sqrt(MA2 - 4*(mass**2))/(2) ) / MA2\n",
    "\n",
    "        # From paper https://arxiv.org/pdf/1505.07459\n",
    "        term = MA\n",
    "        term *= np.sqrt(1 - 4*(mass**2 / MA**2))\n",
    "        term *= (1 + 2*(mass**2 / MA**2))\n",
    "        \n",
    "        total += term\n",
    "        terms.append(term)\n",
    "    terms = np.array(terms)\n",
    "\n",
    "    #terms /= total\n",
    "    #a = np.sqrt(MA**2 - 4*MB**2)\n",
    "    #b = np.sqrt(MA**2 - 4*MC**2)\n",
    "    \n",
    "    #BRA = a/(a+b)\n",
    "    #BRB = b/(a+b)\n",
    "\n",
    "    return terms\n",
    "\n",
    "ma = 0.216744\n",
    "mb = 0.000511\n",
    "mc = 0.105\n",
    "brs = calc_branching_fractions(ma, [mb, mc])\n",
    "output = f\"BR A: {ma:6.3f}  \"\n",
    "for br in brs:\n",
    "    output += f\"{br:.4f} \"\n",
    "output += f\"{sum(brs)}\"\n",
    "print(output)\n",
    "\n",
    "ma = 0.25\n",
    "mb = 0.000511\n",
    "mc = 0.105\n",
    "brs = calc_branching_fractions(ma, [mb, mc])\n",
    "output = f\"BR A: {ma:6.3f}  \"\n",
    "for br in brs:\n",
    "    output += f\"{br:.4f} \"\n",
    "output += f\"{sum(brs)}\"\n",
    "print(output)\n",
    "\n",
    "ma = 0.5\n",
    "mb = 0.000511\n",
    "mc = 0.105\n",
    "brs = calc_branching_fractions(ma, [mb, mc])\n",
    "output = f\"BR A: {ma:6.3f}  \"\n",
    "for br in brs:\n",
    "    output += f\"{br:.4f} \"\n",
    "output += f\"{sum(brs)}\"\n",
    "print(output)\n",
    "\n",
    "\n",
    "ma = 1.0\n",
    "mb = 0.000511\n",
    "mc = 0.105\n",
    "md = 0.139\n",
    "me = 0.494\n",
    "\n",
    "brs = calc_branching_fractions(ma, [mb, mc, md, me])\n",
    "output = f\"BR A: {ma:6.3f}  \"\n",
    "for br in brs:\n",
    "    output += f\"{br:.4f} \"\n",
    "output += f\"{sum(brs)}\"\n",
    "print(output)\n",
    "\n",
    "ma = 5.0\n",
    "mb = 0.000511\n",
    "mc = 0.105\n",
    "md = 1.776\n",
    "\n",
    "brs = calc_branching_fractions(ma, [mb, mc, md])\n",
    "output = f\"BR A: {ma:6.3f}  \"\n",
    "for br in brs:\n",
    "    output += f\"{br:.4f} \"\n",
    "output += f\"{sum(brs)}\"\n",
    "print(output)\n",
    "\n",
    "\n",
    "ma = 3.0\n",
    "#masses = [0.000511, 0.105, 0.139, 0.134, 0.494, 0.494]\n",
    "masses = [0.000511, 0.105, 0.139, 0.134, 0.494, 0.494, 0.938, 0.939]\n",
    "\n",
    "brs = calc_branching_fractions(ma, masses)\n",
    "output = f\"BR A: {ma:6.3f}  \"\n",
    "for br in brs:\n",
    "    output += f\"{br:.4f} \"\n",
    "output += f\"{sum(brs)}\"\n",
    "print(output)\n",
    "\n",
    "\n",
    "\n",
    "#bra, brb = calc_branching_fractions(ma, mb, mc)\n",
    "#print(f\"BR A: {bra}   BR B: {brb}    {bra+brb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilons\n",
    "df_br_muon = pd.read_csv('Branching_Ratio/brto_muon_extracted_from_paper.csv')\n",
    "df_br_muon\n",
    "\n",
    "df_br_electron = pd.read_csv('Branching_Ratio/brtoe.csv')\n",
    "df_br_electron\n",
    "\n",
    "\n",
    "#df_br_muon.plot(x='x (GeV)', y='y (branching ratio)', figsize=(12,4))\n",
    "\n",
    "xe = df_br_electron['mA[GeV]']\n",
    "ye = df_br_electron['BR']\n",
    "\n",
    "xmu = df_br_muon['x (GeV)']\n",
    "ymu = df_br_muon['y (branching ratio)']\n",
    "\n",
    "plt.plot(xe, ye, label='BR(electron)')\n",
    "plt.plot(xmu, ymu, label='BR(muon)')\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "\n",
    "bre_interp = interpolate.interp1d(xe, ye)\n",
    "brmu_interp = interpolate.interp1d(xmu, ymu)\n",
    "\n",
    "\n",
    "xptse = np.linspace(0.1, 10, 1000)\n",
    "xptsmu = np.linspace(0.221, 10, 1000)\n",
    "\n",
    "yptse = bre_interp(xptse)\n",
    "yptsmu = brmu_interp(xptsmu)\n",
    "\n",
    "plt.plot(xptse, yptse, label='interp(electron)')\n",
    "plt.plot(xptsmu, yptsmu, label='interp(electron)')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xptsmu\n",
    "bre_interp(10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kappa['mx'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First order test case\n",
    "\n",
    "# Read in the kappa0 from elsewhere\n",
    "#df_in = pd.read_parquet('different_kappas.parquet')\n",
    "#df_in = pd.read_parquet('different_kappas_03052025_many_mxs.parquet')\n",
    "\n",
    "#df_in = pd.read_parquet('different_kappas_MANY_MX_MASSES.parquet')\n",
    "#df_in = pd.read_parquet('different_kappas_TEST.parquet')\n",
    "\n",
    "dict_results = {}\n",
    "dict_results['mx'] = []\n",
    "dict_results['ma'] = []\n",
    "dict_results['kappa0'] = []\n",
    "dict_results['alphax'] = []\n",
    "dict_results['alpha_therm_or_max'] = []\n",
    "dict_results['BR'] = []\n",
    "dict_results['epsilon'] = []\n",
    "dict_results['cap1'] = []\n",
    "dict_results['sommerfeld'] = []\n",
    "dict_results['rate_1yr'] = []\n",
    "dict_results['rate_CMS_1yr'] = []\n",
    "dict_results['rate_10yrs'] = []\n",
    "dict_results['rate_CMS_10yrs'] = []\n",
    "dict_results['rate_1month'] = []\n",
    "dict_results['rate_CMS_1month'] = []\n",
    "dict_results['livetime_years'] = []\n",
    "dict_results['livetime_seconds'] = []\n",
    "dict_results['depth_scale'] = []\n",
    "dict_results['angular_acceptance'] = []\n",
    "dict_results['final_state_particles'] = []\n",
    "\n",
    "\n",
    "# Fine structure constant, natch\n",
    "alpha = 1./137\n",
    "\n",
    "# Dark fine structure constant\n",
    "#alphax = 0.035\n",
    "\n",
    "#alphax = 0.0001\n",
    "#alphax = 0.17\n",
    "\n",
    "#alphax = 1.0\n",
    "\n",
    "# Age of Earth. Time to accumulate dark matter\n",
    "tauCross = DP.tauCross\n",
    "print(f\"tauCross: {tauCross:.2e} seconds\")\n",
    "print(f\"tauCross: {tauCross/3.1e7:.2e} years\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "#mxs = [1000]\n",
    "#mxs = [100]\n",
    "#mxs = [10, 100, 1000, 10000]\n",
    "\n",
    "# For the full range\n",
    "'''\n",
    "mxs = np.linspace(10, 90, 9).tolist()\n",
    "mxs += np.linspace(100, 900, 9).tolist()\n",
    "mxs += np.linspace(1000, 9000, 9).tolist()\n",
    "mxs += np.linspace(10000, 90000, 9).tolist()\n",
    "mxs += np.linspace(100000, 1000000, 10).tolist()\n",
    "mass_tag = 'ALL_MASSES'\n",
    "'''\n",
    "\n",
    "# For finer grained\n",
    "mxs = df_kappa['mx'].values\n",
    "mass_tag = 'MX_1000_to_10000'\n",
    "\n",
    "#print(mxs)\n",
    "print(f\"Running over {len(mxs)} values of m_X from {min(mxs)} to {max(mxs)}\\n\")\n",
    "\n",
    "#mas = [1.0]\n",
    "#mas = np.linspace(0.25, 10, 2500)\n",
    "mas = np.linspace(0.01, 10, 250)\n",
    "\n",
    "mas = np.linspace(0.01, 0.09, 450).tolist() + np.linspace(0.1, 0.9, 450).tolist() + np.linspace(1, 10, 450).tolist()\n",
    "#print(mas)\n",
    "\n",
    "print(f\"Running over {len(mas)} values of m_a from {min(mas)} to {max(mas)}\\n\")\n",
    "\n",
    "#epsilons = [1e-7, 1e-8, 1e-9]\n",
    "#epsilons = np.linspace(1e-9, 1e-7, 10)\n",
    "epsilons = np.linspace(1e-11, 9e-11, 9).tolist() \n",
    "epsilons = np.linspace(1e-10, 9e-10, 9).tolist() \n",
    "epsilons += np.linspace(1e-9, 9e-9, 9).tolist()\n",
    "epsilons += np.linspace(1e-8, 9e-8, 9).tolist()\n",
    "epsilons += np.linspace(1e-7, 9e-7, 9).tolist()\n",
    "#epsilons += np.linspace(1e-6, 9e-6, 9).tolist()\n",
    "\n",
    "#epsilons = [1e-7]\n",
    "\n",
    "print(f\"Running over {len(epsilons)} values of m_a from {min(epsilons)} to {max(epsilons)}\\n\")\n",
    "\n",
    "#live_time = 1/12 # years\n",
    "live_time = 1 # years\n",
    "\n",
    "# For testing\n",
    "#live_time = 10  # years\n",
    "\n",
    "start = time.time()\n",
    "for mx in mxs:\n",
    "\n",
    "    # GET KAPPA0\n",
    "    #kappa0 = kappa0_vals[mx]\n",
    "    filter = df_kappa['mx'] == mx\n",
    "    kappa0 = df_kappa[filter]['kappa0'].values[0]\n",
    "\n",
    "\n",
    "    time_to_run = time.time() - start\n",
    "    print(f\"mx: {mx}    kappa0: {kappa0:.2e}     time to run: {time_to_run:.2f} s\")\n",
    "    start = time.time()\n",
    "\n",
    "    for ma in mas:\n",
    "        # This would give us an error in the alphaThermApprox calculation\n",
    "        if ma == mx:\n",
    "            continue\n",
    "\n",
    "        #print(f\"mx: {mx}    ma: {ma}\")\n",
    "        \n",
    "        # Calculate it from the thermal relics\n",
    "        #print(f\"here: {mx}  {ma}\")\n",
    "        alphax_thermal_relics = DP.alphaTherm(mx, ma)\n",
    "        alphax_max = 0.17 * (mx/1000)**1.61\n",
    "        #alphax = alphax_max\n",
    "\n",
    "        for alphax,therm_or_max in zip([alphax_thermal_relics, alphax_max], ['THERMAL', 'MAX']):\n",
    "            #thermAvgSomm = DP.thermAvgSommerfeld(mx, mA, alpha_X)\n",
    "            sommerfeld = DP.thermAvgSommerfeld(mx,ma,alphax)\n",
    "            \n",
    "            sigma = DP.sigmaVtree(mx,ma,alphax)\n",
    "            \n",
    "            ann = DP.cAnn(mx,sigma,sommerfeld)\n",
    "    \n",
    "            # Branching fractions\n",
    "            #branching_fraction_to_electrons = 1\n",
    "            #branching_fraction_to_muons = 0.3\n",
    "            \n",
    "            #mb = 0.000511\n",
    "            #mc = 0.105\n",
    "            #br_electrons, br_muons = calc_branching_fractions(ma, mb, mc)\n",
    "            \n",
    "            br_electrons = bre_interp(ma)\n",
    "            \n",
    "            br_muons = 0\n",
    "            if ma>0.22:\n",
    "                br_muons = brmu_interp(ma)\n",
    "            \n",
    "            #branching_fraction_to_muons = br_muons\n",
    "            \n",
    "            #print(f\"branching fraction to muons: {branching_fraction_to_muons}\")\n",
    "            \n",
    "            #branching_fraction_to_final_state_particles = br_electrons\n",
    "            #branching_fraction_to_final_state_particles = br_muons\n",
    "            #####################################################################\n",
    "    \n",
    "            # Scale\n",
    "            # Do a rough area scale\n",
    "            angular_acceptance_scale = (20**2)/(1000**2)\n",
    "            \n",
    "            # Do a rough depth scale, based on the energy loss in rock\n",
    "            depth_scale = 1\n",
    "            if mx == 10:\n",
    "                depth_scale = 10 / 1000\n",
    "            elif mx == 100:\n",
    "                depth_scale = 100 / 1000\n",
    "            else:\n",
    "                depth_scale = 1000/1000\n",
    "    \n",
    "            for branching_fraction_to_final_state_particles, final_state_particles in zip([br_muons, br_electrons],['muons', 'electrons']):\n",
    "        \n",
    "                for epsilon in epsilons:\n",
    "        \n",
    "                    if branching_fraction_to_final_state_particles > 0:\n",
    "                        cap1 = DP.cCapQuick(mx, ma, epsilon, alphax, kappa0)\n",
    "                                   \n",
    "                        tau = DP.tau(cap1, ann)\n",
    "                        \n",
    "                        gammaAnn = DP.gammaAnn(cap1,ann)\n",
    "                                    \n",
    "                        L = DP.decayLength(mx,ma,epsilon,branching_fraction_to_final_state_particles)\n",
    "                        \n",
    "                        Edecay = DP.epsilonDecay(L)\n",
    "                        \n",
    "                        signal = DP.iceCubeSignal(gammaAnn,Edecay,DP.yr2s(live_time))\n",
    "                    \n",
    "                    else:\n",
    "                        tau = 0\n",
    "                        gammaAnn = 0\n",
    "                        L = 0\n",
    "                        Edecay = 0\n",
    "                        signal = 0\n",
    "                        cap1 = 0\n",
    "                                    \n",
    "                    signal_CMS = signal*angular_acceptance_scale*depth_scale\n",
    "                    \n",
    "                    #print(signal)\n",
    "                    #if signal>1e9:\n",
    "                    #  print(f\"mx: {mx:5d}  ma: {ma:4.3f}   epsilon: {epsilon:.2e}     kappa0: {kappa0:.2e}   # signal: {signal:10.2f}  # CMS signal: {signal_CMS:.2f}\")\n",
    "        \n",
    "                    # For some reason this was getting saved as an object at times. \n",
    "                    br = branching_fraction_to_final_state_particles\n",
    "                    try:\n",
    "                        float(br)\n",
    "                        branching_fraction_to_final_state_particles = float(branching_fraction_to_final_state_particles)\n",
    "                    except ValueError:\n",
    "                        print(br)\n",
    "                        print(f\"mx: {mx:5d}  ma: {ma:4.3f}   epsilon: {epsilon:.2e}     kappa0: {kappa0:.2e}   # signal: {signal:10.2f}  # CMS signal: {signal_CMS:.2f}\")\n",
    "        \n",
    "                        print(\"Not a float\")\n",
    "                    \n",
    "                    dict_results['mx'].append(mx)\n",
    "                    dict_results['ma'].append(ma)\n",
    "                    dict_results['kappa0'].append(kappa0)\n",
    "                    dict_results['alphax'].append(alphax)\n",
    "                    dict_results['BR'].append(branching_fraction_to_final_state_particles)\n",
    "                    dict_results['epsilon'].append(epsilon)\n",
    "                    dict_results['cap1'].append(cap1)\n",
    "                    dict_results['sommerfeld'].append(sommerfeld)\n",
    "                    dict_results['rate_1yr'].append(signal)\n",
    "                    dict_results['rate_CMS_1yr'].append(signal_CMS)\n",
    "                    dict_results['rate_1month'].append(signal/12)\n",
    "                    dict_results['rate_CMS_1month'].append(signal_CMS/12)\n",
    "                    dict_results['rate_10yrs'].append(signal*10)\n",
    "                    dict_results['rate_CMS_10yrs'].append(signal_CMS*10)\n",
    "                    dict_results['livetime_years'].append(live_time)\n",
    "                    dict_results['livetime_seconds'].append(DP.yr2s(live_time))\n",
    "                    dict_results['alpha_therm_or_max'].append(therm_or_max)\n",
    "                    dict_results['depth_scale'].append(depth_scale)\n",
    "                    dict_results['angular_acceptance'].append(angular_acceptance_scale)\n",
    "                    dict_results['final_state_particles'].append(final_state_particles)\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = 2000\n",
    "filter = df_kappa['mx'] == mx\n",
    "kappa0 = df_kappa[filter]['kappa0'].values[0]\n",
    "kappa0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'rates_electrons_alpha_max_one_month.parquet'\n",
    "#filename = 'rates_muons_alpha_max_one_month.parquet'\n",
    "#filename = 'rates_muons_alpha_max_one_month_HIGH_MASSES.parquet'\n",
    "#filename = 'rates_muons_alpha_max_one_month_ALL_MASSES.parquet'\n",
    "#filename = 'rates_muons_electrons_both_alphas_ALL_MASSES.parquet'\n",
    "filename = f'rates_muons_electrons_both_alphas_{mass_tag}.parquet'\n",
    "\n",
    "#filename = 'rates_muons_alpha_therm_one_month.parquet'\n",
    "#filename = 'rates_electrons_alpha_therm_one_month.parquet'\n",
    "df_results.to_parquet(filename)\n",
    "\n",
    "\n",
    "#filename = 'rates_muons_alpha_max_one_month_HIGH_MASSES.parquet'\n",
    "#df_results_HM = pd.read_parquet(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results.plot.scatter(x='mx', y='cap1')\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for br in df_results.BR:\n",
    "    try:\n",
    "        float(br)\n",
    "    except ValueError:\n",
    "        print(br)\n",
    "        print(\"Not a float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SEEMS TO TAKE A REALLY LONG TIME WITH THE BIG FILE\n",
    "'''\n",
    "plt.figure(figsize=(16,4))\n",
    "sns.scatterplot(df_results, x='ma', y='sommerfeld', hue='mx')\n",
    "plt.ylim(0.1)\n",
    "plt.yscale('log')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'rates_muons_alpha_therm_one_month.parquet'\n",
    "\n",
    "#df_results = pd.read_parquet(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df_results, col='epsilon', hue='mx',  col_wrap=4, height=2)\n",
    "g.map(sns.scatterplot, \"ma\", 'rate_CMS_1month')\n",
    "#g.map(sns.scatterplot, \"ma\", 'rate')\n",
    "\n",
    "# Need this to get the labels not crazy with too many decimal places\n",
    "g.set_titles(template=\"epsilon={col_name:.0e}\")\n",
    "\n",
    "g.add_legend()\n",
    "\n",
    "#plot = sns.scatterplot(df_results, x='ma', y='rate_CMS', hue='mx', style='epsilon', palette=sns.color_palette('tab10', n_colors=4), s=100)#, fmt='f')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylim(0.1,10000000)\n",
    "plt.xlim(0,2)\n",
    "\n",
    "plt.savefig('many_mxs_03062025.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'rates_electrons_alpha_therm_one_month.parquet'\n",
    "#filename = 'rates_muons_alpha_therm_one_month.parquet'\n",
    "#filename = 'rates_muons_alpha_max_one_month.parquet'\n",
    "#filename = 'rates_muons_alpha_max_one_month_ALL_MASSES.parquet'\n",
    "\n",
    "filename = 'rates_muons_electrons_both_alphas_ALL_MASSES.parquet'\n",
    "\n",
    "\n",
    "#filename = 'rates_electrons_alpha_therm_one_month.parquet'\n",
    "\n",
    "df_results = pd.read_parquet(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate = 10*12*df_results['rate']\n",
    "#rate = 10*12*df_results['rate_CMS']\n",
    "\n",
    "#rate = df_results['rate_CMS_1month']\n",
    "\n",
    "#final_state_particles = 'muons'\n",
    "final_state_particles = 'electrons'\n",
    "\n",
    "alphax_assumption = 'MAX'\n",
    "live_time = '10yrs'\n",
    "#live_time = '1month'\n",
    "\n",
    "rate_detector = 'rate'\n",
    "#rate_detector = 'rate_CMS'\n",
    "\n",
    "filter = (df_results['final_state_particles']==final_state_particles)\n",
    "filter = filter & (df_results['alpha_therm_or_max']==alphax_assumption)\n",
    "\n",
    "df_tmp = df_results[filter]\n",
    "\n",
    "rate_string = f\"{rate_detector}_{live_time}\"\n",
    "print(f\"Plotting for rate {rate_string}\")\n",
    "rate = df_tmp[rate_string]\n",
    "mx_vals = df_tmp['mx']\n",
    "\n",
    "x = df_tmp['ma']\n",
    "y = df_tmp['epsilon']\n",
    "\n",
    "plt.figure(figsize=(16,15))\n",
    "\n",
    "\n",
    "mxs = [10, 100, 1000, 10000]\n",
    "#mxs = [1000, 10000, 100000, 1000000]\n",
    "\n",
    "for idx,test_mx in enumerate(mxs):\n",
    "#for idx,test_mx in enumerate([10, 100, 1000, 10000]):\n",
    "#for idx,test_mx in enumerate([1000, 10000, 100000, 1000000]):\n",
    "\n",
    "    print(f\"{idx} {test_mx}\")\n",
    "\n",
    "    plt.subplot(2,2,idx+1)\n",
    "    \n",
    "    for expected_rate in [1, 10, 100, 1000]:\n",
    "        \n",
    "        filter = (test_mx == mx_vals) & (np.abs(rate-expected_rate)/expected_rate<0.3)\n",
    "        #filter = filter & (df_results['final_state_particles']=='electrons')\n",
    "        #filter = filter & (df_results['alpha_therm_or_max']=='MAX')\n",
    "\n",
    "        #x = df_tmp['ma']\n",
    "        #y = df_tmp['epsilon']\n",
    "        \n",
    "        plt.plot(x[filter], y[filter], '.', label=f'rate={expected_rate}')\n",
    "\n",
    "        #print(test_mx, expected_rate, df_results[filter]['rate_CMS'])\n",
    "\n",
    "    plt.xlim(0.01, 10)\n",
    "    plt.ylim(1e-11, 1e-6)\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(r\"$m_{A'}$ [GeV]\", fontsize=18)\n",
    "    plt.ylabel(r\"$\\epsilon$\", fontsize=18)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.title(f'$M_X = {int(test_mx):d}$ GeV', fontsize=18)\n",
    "#plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"Fig3_comparison_10_years_IceCube_electrons.png\")\n",
    "#plt.savefig(\"Fig3_comparison_10_years_IceCube_muons.png\")\n",
    "#plt.savefig(\"Fig3_comparison_1_month_CMS_electrons.png\")\n",
    "\n",
    "#plt.savefig(\"Fig3_comparison_1_month_CMS_muons_alpha_therm.png\")\n",
    "\n",
    "#plt.savefig(\"Fig3_comparison_1_month_CMS_muons_alpha_max_high_masses.png\")\n",
    "file_string = f\"Fig3_comparison_{rate_string}_{final_state_particles}_alphax_{alphax_assumption}_mX_range_{mxs[0]}_{mxs[-1]}.png\"\n",
    "plt.savefig(file_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = df_results['rate_10yrs']\n",
    "\n",
    "mx_vals = df_results['mx']\n",
    "\n",
    "test_mx = 100\n",
    "\n",
    "filter = (np.abs(rate-1000)<50) & (test_mx == mx_vals)\n",
    "\n",
    "\n",
    "\n",
    "df_results[filter]#.plot.scatter(x='ma', y='BR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.plot.scatter(x='mx', y='rate_CMS_1yr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import LinearNDInterpolator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphax = df_results['alphax']\n",
    "\n",
    "alphax.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mx = 100\n",
    "\n",
    "mx = df_results['mx']\n",
    "filter = (test_mx == mx_vals)\n",
    "\n",
    "ma = df_results['ma'][filter]\n",
    "alphax = df_results['alphax'][filter]\n",
    "\n",
    "plt.plot(ma, alphax, '.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#rng = np.random.default_rng()\n",
    "\n",
    "test_mx = 1000\n",
    "\n",
    "mx = df_results['mx']\n",
    "\n",
    "alphax = df_results['alphax']\n",
    "\n",
    "filter = (test_mx == mx_vals)\n",
    "\n",
    "ma = df_results['ma'][filter]\n",
    "epsilon = df_results['epsilon'][filter]\n",
    "rate = df_results['rate'][filter]\n",
    "\n",
    "#x = rng.random(10) - 0.5\n",
    "#y = rng.random(10) - 0.5\n",
    "#z = np.hypot(x, y)\n",
    "\n",
    "x = ma\n",
    "y = epsilon\n",
    "z = rate\n",
    "\n",
    "min_rate = rate>1\n",
    "\n",
    "X = np.linspace(min(x), max(x))\n",
    "Y = np.linspace(min(y), max(y))\n",
    "X, Y = np.meshgrid(X, Y)  # 2D grid for interpolation\n",
    "\n",
    "interp = LinearNDInterpolator(list(zip(x, y)), z)\n",
    "Z = interp(X, Y)\n",
    "\n",
    "#plt.pcolormesh(X, Y, Z, shading='auto')\n",
    "plt.scatter(x[min_rate], y[min_rate], marker=\"o\", c=np.log10(z[min_rate]), cmap='viridis', label=\"input point\")\n",
    "#plt.scatter(x[min_rate], y[min_rate], marker=\"o\", c=z[min_rate], cmap='viridis', label=\"input point\")\n",
    "\n",
    "#plt.legend()\n",
    "plt.colorbar()\n",
    "\n",
    "plt.ylim(1e-11, 1e-6)\n",
    "plt.xlim(0.01,10)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x\n",
    "y\n",
    "#z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[min_rate],z[min_rate],'.')\n",
    "plt.ylim(0.1)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['epsilon'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_vals = df_results['mx']\n",
    "test_mx = 100\n",
    "filter = (test_mx == mx_vals)\n",
    "\n",
    "df_results[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching ratios from their paper\n",
    "\n",
    "df_br = pd.read_csv('Branching_Ratio/brtoe.csv')\n",
    "df_br\n",
    "\n",
    "df_br.plot(x='mA[GeV]', y='BR')\n",
    "df_results.plot.scatter(x='ma', y='BR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equilibrium Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recipe for this section is:\n",
    "\n",
    "1. Read in a completed $\\texttt{Sommerfeld.csv}$ file\n",
    "2. Extract contour data\n",
    "3. Plot contours of constant $\\tau/\\tau_\\oplus$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read In Sommerfeld File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell reads in a $\\texttt{Sommerfeld.csv}$ file and interpolates the discrete set of $\\langle S_S (m_{A'})\\rangle$ points into a continuous function. To save computation time, we call this interpolation during the calculation of the equilibrium time as opposed to calling the function `DarkCapPy.DarkPhton.thermAvgSommerfeld` explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:30:51.113305Z",
     "start_time": "2019-06-19T20:30:48.669219Z"
    }
   },
   "outputs": [],
   "source": [
    "#sommFileName = input('Sommerfeld Data File: ')\n",
    "sommFileName = '100GeVSommerfeld.csv'\n",
    "sommFile = sommerfeldPath(sommFileName)\n",
    "dataIn = pd.read_csv(sommFile, sep = ',')\n",
    "\n",
    "Filem_X = float(dataIn.at[0,'mX[GeV]'])\n",
    "FileKappa0 = float(dataIn.at[0,'Kappa0[GeV5]'])\n",
    "print(FileKappa0)\n",
    "\n",
    "###############################\n",
    "# Set the correct units for m_X\n",
    "###############################\n",
    "m_XUnit = 'None'\n",
    "\n",
    "if (Filem_X < 1000):\n",
    "    m_XDisplay = Filem_X\n",
    "    m_XUnit = 'GeV'\n",
    "\n",
    "if (Filem_X >= 1000):\n",
    "    m_XDisplay = Filem_X*10**-3\n",
    "    m_XUnit = 'TeV'\n",
    "    \n",
    "\n",
    "# Double Check to make sure the file name matches the read-in value\n",
    "print ('Input m_X: {0} GeV'.format(Filem_X))\n",
    "\n",
    "\n",
    "###############################\n",
    "# Interpolate Sommerfeld\n",
    "###############################\n",
    "maList = dataIn['mA[GeV]']\n",
    "sommerfeldList = dataIn['ThermAvgSommerfeld']\n",
    "\n",
    "# Deprecated\n",
    "SommerfeldInterp = interpolate.interp1d(maList, sommerfeldList)\n",
    "\n",
    "\n",
    "print ('Compete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T16:40:56.634659Z",
     "start_time": "2018-03-27T16:40:56.631623Z"
    }
   },
   "source": [
    "## Extract Equilibrium Contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Included in this package is the function `tau` = $\\tau = \\sqrt{C_\\text{cap}C_\\text{ann}}$, which is required to plot contours of equilibrium time. The relevant ratio is is the equilibrium time relative to the age of the Earth, $\\tau_\\oplus$. However, generating contours of constant $\\tau/\\tau_\\oplus$ using `matplotlib.contour` is computationally inefficient. Instead, it is much better to define a contour level by its order of magnitude, $L$ in $\\tau/\\tau_\\oplus = 10^L$, so that:\n",
    "\n",
    "$$ \\varepsilon(m_{A'}) = 2\\log(m_{A'}) -\\frac{1}{2}\\log(\\alpha_X C_\\text{ann,0} \\langle S_S \\rangle) - \\frac{1}{2}\\log(\\kappa_0) - \\log(\\mathrm{L}  \\tau_{\\oplus}) $$\n",
    "\n",
    "For a constant contour $L$, dark matter mass $m_X$, and dark matter fine structure constant $\\alpha_X$, the last two terms are constant in $(\\varepsilon, m_{A'})$ space.\n",
    "\n",
    "\n",
    "The following cell initializes `contourDictionary`, a dictionary which stores arrays of $\\varepsilon(m_{A'}$) values for different contours. The keys for `contourDictionary` are contour levels $L$ corresponding to the $\\tau/\\tau_\\oplus = 10^\\mathrm{L}$ contour. The values of `contourDictionary` are an array of two sub-arrays. The first sub-array stores $\\varepsilon(m_{A'})$ with no Sommerfeld enhancements ($\\langle S_S \\rangle =1$). The second sub-array stores $\\varepsilon(m_{A'})$ with Sommerfeld enhancements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:30:55.859979Z",
     "start_time": "2019-06-19T20:30:55.417015Z"
    }
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# Create contourDictionary\n",
    "####################################\n",
    "# The keys of contourDictionary define the 10^key contour level\n",
    "# the values are an array of plot values: [[],[]] \n",
    "# The first array is Sommerfeld off, the second array is sommerfeld on.\n",
    "contourDictionary={\n",
    "    '-4':[[],[]],\n",
    "    '-2':[[],[]],\n",
    "    '0' :[[],[]],\n",
    "    '2' :[[],[]],\n",
    "    '4' :[[],[]]\n",
    "}\n",
    "\n",
    "for key in contourDictionary:\n",
    "    contourDictionary[key] = [[],[]]\n",
    "\n",
    "####################################\n",
    "# Initialize the x-axis\n",
    "# 0.01 GeV < m_A < 10 GeV\n",
    "####################################\n",
    "maMin = min(maList)\n",
    "maMax = max(maList)\n",
    "\n",
    "maRange = np.logspace(np.log10(maMin), np.log10(maMax), 500, base = 10)\n",
    "\n",
    "####################################\n",
    "# Populate contourDictionary with plot values\n",
    "####################################\n",
    "maArray = []\n",
    "\n",
    "for maTemp in maRange:\n",
    "    maArray.append(maTemp)\n",
    "\n",
    "for key in contourDictionary:\n",
    "    contourline = float(key)\n",
    "    \n",
    "    for mATemp in maRange:\n",
    "        Alpha_X    = DP.alphaTherm(Filem_X, mATemp)\n",
    "        #print(Alpha_X)\n",
    "        SigmaVTree = DP.sigmaVtree(Filem_X, mATemp, Alpha_X)\n",
    "        Sommerfeld = SommerfeldInterp(mATemp)\n",
    "        CannNoSomm = DP.cAnn(Filem_X, SigmaVTree)\n",
    "        #print(Sommerfeld)\n",
    "\n",
    "        #print(FileKappa0)\n",
    "        EpsilonNoSomm = DP.contourFunction(mATemp, Alpha_X, CannNoSomm, 1, FileKappa0, contourline)\n",
    "        EpsilonSomm   = DP.contourFunction(mATemp, Alpha_X, CannNoSomm, Sommerfeld, FileKappa0, contourline)\n",
    "\n",
    "        #print(EpsilonSomm)\n",
    "    \n",
    "        contourDictionary[key][0].append(EpsilonNoSomm)\n",
    "        contourDictionary[key][1].append(EpsilonSomm)\n",
    "\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contourDictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Equilibrium Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell produces a single plot of the equilibrium time. The user must specify whether the Sommerfeld enhancement is \"on\" or \"off\" by following the prompt. We also define a custom color scheme so that the contours follow a color gradient as opposed to the default `matplotlib` colors which are uncoordinated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:31:08.242927Z",
     "start_time": "2019-06-19T20:31:01.304781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define custom Color scheme\n",
    "colorScheme1 = mpl.cycler(color=['#c7c47e','#94ab72','#428583','#40379f','#3d0b7e']) \n",
    "\n",
    "EQFig = plt.figure(figsize = (6,6))\n",
    "\n",
    "sommParam = int(input('Sommerfeld on? No:0 Yes:1 '))\n",
    "assert ((sommParam == 0) or (sommParam == 1)), 'Invalid Input, must be 0 or 1.'\n",
    "\n",
    "\n",
    "########################################\n",
    "# This uses a custom color scheme defined in Masterfunctions_Graphing\n",
    "########################################\n",
    "colors = colorScheme1\n",
    "mpl.rcParams['axes.prop_cycle'] = colors\n",
    "\n",
    "########################################\n",
    "# Produce the contour lines\n",
    "########################################\n",
    "testFill = False\n",
    "for key in contourDictionary:\n",
    "    if (float(key) == 0):\n",
    "        testFill = True\n",
    "    assert (len(maRange) == len(contourDictionary[key][sommParam])), 'ContourDictionary is probably not populated'\n",
    "    plt.plot(np.log10(maRange), (contourDictionary[key][sommParam]), label = r'10^{0}'.format(key))\n",
    "    \n",
    "\n",
    "########################################  \n",
    "# Shade the valid region of parameter space   \n",
    "########################################\n",
    "if (testFill == True):\n",
    "    xs = np.log10(maArray)\n",
    "    y1 = contourDictionary['0'][sommParam]\n",
    "    plt.fill_between(xs, y1, -5, color = '#E6FDFF')\n",
    "\n",
    "########################################\n",
    "# Define plot label stuff\n",
    "########################################\n",
    "plt.legend(loc = 'lower right', fontsize = 12)\n",
    "plt.xlabel(r\"$\\log(m_{A'})$ $[GeV]$\", fontsize = 14)\n",
    "plt.ylabel(r'$\\log(\\varepsilon)$', fontsize = 14)\n",
    "\n",
    "plt.title(r'$m_X = {0}$ {1}'.format(m_XDisplay, m_XUnit),\\\n",
    "          fontsize = 16,\\\n",
    "          loc = 'right')\n",
    "\n",
    "plt.title(r'$\\tau/\\tau_{\\oplus}$ Contours',\\\n",
    "          fontsize = 16,\\\n",
    "          loc = 'left')\n",
    "plt.axis([np.log10(min(maRange)),np.log10(max(maRange)),-10,-5])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T16:33:21.352510Z",
     "start_time": "2018-08-09T16:33:16.392844Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = input('Figure Name: ')\n",
    "assert (filename != ''), 'No Filename'\n",
    "EQFig.savefig(filename, dpi = 700)\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IceCube Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recipe for generating the $\\texttt{Signal.csv}$ is:\n",
    "\n",
    "1. Read in $\\texttt{Sommerfeld.csv}$ and $\\texttt{Branch.csv}$\n",
    "\n",
    "2. Define the resolution of the signal plot\n",
    "\n",
    "3. Check this resolution \n",
    " \n",
    "4. Initialize a template dataframe to hold the signal rate calculations\n",
    "\n",
    "5. Generate $\\texttt{Signal.csv}$\n",
    "\n",
    "6. **This step is time consuming and computationally intensive.** <BR> Loop through signalData.csv, calculate $C_{Cap}$, $C_{Ann}$, $\\Gamma_{Ann}$, and $N_{Sig}$, append them to signalData.csv\n",
    "\n",
    "Step six can be quite lengthy depending on the desired resolution of the data points. Typically, one would begin these calculations and be forced to wait until they are completed in their entirety; perhaps unable to shut down their machine for hours at a time. However, the cell \"Signal.csv Calculations\" can be interrupted at any time, by interrupting the Jupyter kernel, and resumed later. This allows the user to run the calculation in multiple stages instead of dedicating a large chunk of time and running the calculation all at once. If the calculation is interrupted, run the \"Manual Overwrite\" cell to guarentee that the dataframe stored in Jupyter memory is written to $\\texttt{Signal.csv}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Signal.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in External Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, the user inputs two files:\n",
    "\n",
    "1. A completed $\\texttt{Sommerfeld.csv}$\n",
    "2. A csv containing the branching ratio for a given dark photon decay process, $\\texttt{Branch.csv}$\n",
    "\n",
    "We provide the file `brtoe.csv` for the process $A' \\rightarrow e^+e^-$ from [arXiv:1505.07459](https://arxiv.org/abs/1505.07459)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:33:01.014049Z",
     "start_time": "2019-06-19T20:32:51.019198Z"
    }
   },
   "outputs": [],
   "source": [
    "SommFileName = input('Sommerfeld File: ')\n",
    "SommFile = pd.read_csv(sommerfeldPath(SommFileName))\n",
    "\n",
    "branchFileName = (input('Branching File: '))\n",
    "branchFile = pd.read_csv(branchPath(branchFileName))\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Resolution of the Signal Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the resolution of the signal plot and consequently for example the sampling of the Sommerfeld enhancement. \n",
    "\n",
    "This cell initializes the variable `num_Signal` which represents the resolution of the Signal plot. If If `num_Signal = 50`, the signal plot will have a $50 \\times 50$ resolution. Consequently, the Sommerfeld enhancement and branching ratio interoplations will also have `num_Signal` sampling points. Ideally, the resolution would be very large, `num_Signal ~ 2000`, but the number of calculations for the signal plot scales as `num_Signal`$^2$. There is a clear tradeoff between resolution and computation time. Therefore, it is useful to tune the resolution as needed in order to minimize calculation time. See [Sommerfeld Data Interpolation](#Sommerfeld-Data-Interpolation) for a more detailed discussion of tuning the resolution.\n",
    "\n",
    "For the provided $\\texttt{Sommerfeld.csv}$ file titled `100GeVSommerfeld.csv`, the suggested resolution is `num_Signal = 200`. As `m_x` increases, the Sommerfeld resonances become more closely spaced and `num_Signal` must increase to capture these finer resonance peaks.\n",
    "\n",
    "To demonstrate what `num_Signal` controls, read in `100GeVSommerfeld.csv` in the cell above and compare the Sommerfeld plots with `num_Signal = 40` to `num_Signal = 200`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:33:03.794059Z",
     "start_time": "2019-06-19T20:33:03.780766Z"
    }
   },
   "outputs": [],
   "source": [
    "num_Signal = 10\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External File Interpolations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates interpolations of the:\n",
    "\n",
    "1. Sommerfeld enhancement\n",
    "\n",
    "2. Branching ratio\n",
    "\n",
    "The file names for both of these files is written to the $\\texttt{Signal.csv}$ file. This facilitates resuming the calculation if it is interrupted. If the main calculation cell is interrupted and the Jupyter kernel is closed, all of the variables loaded into Jupyter memory are wiped. Once the caluclation resumes for a given $\\texttt{Signal.csv}$ file, the user must load the particular $\\texttt{Sommerfeld.csv}$ and $\\texttt{Branch.csv}$ files back into Jupyter memory. Writing both of these file names into the $\\texttt{Signal.csv}$ file solves this problem, allowing the user to resume the calculation by running a single cell as opposed to multiple cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sommerfeld Data Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we generate two plots of the Sommerfeld enhancement to help inform an appropriate value for `num_Signal`. \n",
    "\n",
    "To tune the resolution parameter, we focus on the Sommerfeld enhancement. This is because the Sommerfeld enhancement contains narrow resonance peaks that are not be accurately captured in the interpolation unless the resolution is adequate. If `num_Signal` is too low, the sampling yields an artifically jagged enhancement. To illustrage this point, we generate two graphs of the Sommerfeld enhancement. The left plot is generated with a sampling of `num_Signal` points, the right plot is generated using all points from the $\\texttt{Sommerfeld.csv}$ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:33:13.360697Z",
     "start_time": "2019-06-19T20:33:12.373984Z"
    }
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# Interpolate Sommerfeld data\n",
    "####################################\n",
    "mAData = SommFile['mA[GeV]']\n",
    "SommData = SommFile['ThermAvgSommerfeld']\n",
    "SommInterp = interpolate.interp1d(mAData,SommData)\n",
    "\n",
    "####################################\n",
    "# Create Comparison Plots\n",
    "####################################\n",
    "m_XSommerfeld = SommFile.loc[0,'mX[GeV]']\n",
    "kappa0Sommerfeld = SommFile.loc[0,'Kappa0[GeV5]']\n",
    "\n",
    "\n",
    "vect1 = [] # To store m_A values\n",
    "vect2 = [] # To store Sommerfeld values\n",
    "for ma in np.logspace(np.log10(min(mAData)),np.log10(max(mAData)), num_Signal, base = 10):\n",
    "    vect1.append(ma)\n",
    "    vect2.append(SommInterp(ma))\n",
    "        \n",
    "        \n",
    "\n",
    "fig1 = plt.figure(figsize = (14,6))\n",
    "plt.suptitle('Comparison of Sommerfeld Resolutions', fontsize = 18)\n",
    "\n",
    "ax1 = fig1.add_subplot(1,2,1)\n",
    "ax1.plot(np.log10(vect1),np.log10(vect2),color = '#1f77b4')\n",
    "ax1.set_xlabel(r\"$\\log(m_{A'})$ [GeV]\", fontsize = 14)\n",
    "ax1.set_ylabel(r\"$\\log(\\langle S_S \\rangle)$\", fontsize = 14)\n",
    "ax1.set_title('num_Signal = {0} Resolution'.format(num_Signal), fontsize = 14, loc = 'right')\n",
    "plt.grid()\n",
    "\n",
    "ax2 = fig1.add_subplot(1,2,2)\n",
    "ax2.plot(np.log10(mAData), np.log10(SommData), color = '#1f77b4')\n",
    "ax2.set_xlabel(r\"$\\log(m_{A'})$ [GeV]\", fontsize = 14)\n",
    "ax2.set_ylabel(r\"$\\log(\\langle S_S \\rangle)$\", fontsize = 14)\n",
    "ax2.set_title('num_Somm = 2500 Resolution', fontsize = 14, loc = 'right')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Branching Ratio Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell reads in $\\texttt{Branch.csv}$ and create an interpolation of the branching ratio as a function of mediator mass. The branching ratio file provided, `brtoe.csv` was created by taking Figure 2 from [arXiv:1505.07459](https://arxiv.org/abs/1505.07459) and creating a csv file using [Web Plot Digitizer](https://automeris.io/WebPlotDigitizer/).\n",
    "\n",
    "Similar to the sampling of the Sommerfeld effect, the sampling of the branching ratio also depends on `num_Signal`. In the following cell, we provide output similar to the Sommerfeld plots. The left plot is the branching ratio using a sampling of `num_Singal` points. The plot on the right uses all points in the $\\texttt{Branch.csv}$ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:33:19.723879Z",
     "start_time": "2019-06-19T20:33:18.739549Z"
    }
   },
   "outputs": [],
   "source": [
    "xListBranch = branchFile['mA[GeV]']\n",
    "yListBranch = branchFile['BR']\n",
    "\n",
    "branchRatioInterp = interpolate.interp1d(xListBranch, yListBranch)\n",
    "\n",
    "############################\n",
    "# Number of data points\n",
    "############################\n",
    "num_Branch = len(xListBranch)\n",
    "\n",
    "\n",
    "############################\n",
    "# Set low and high m_A values for plotting\n",
    "############################\n",
    "m_AHigh = max(xListBranch)\n",
    "\n",
    "# Get around Branching ratio starting at m_A = 0 GeV\n",
    "if (min(xListBranch) == 0):\n",
    "    m_ALow = 10**-1 # ArXiV Fig 2 starts at m_A = 0.1 GeV so we start there as well\n",
    "\n",
    "else:\n",
    "    m_ALow = min(xListBranch)\n",
    "    \n",
    "    \n",
    "############################\n",
    "# Create data for plots\n",
    "############################\n",
    "vect1 = [] # To store m_A values\n",
    "vect2 = [] # To store Branching ratio values\n",
    "for ma in np.logspace( np.log10(m_ALow), np.log10(m_AHigh), num_Signal, base = 10):\n",
    "    vect1.append(ma)\n",
    "    vect2.append(branchRatioInterp(ma))\n",
    "\n",
    "\n",
    "fig2 = plt.figure(figsize = (14,6))\n",
    "plt.suptitle('Comparison of Branching Ratio Resolutions', fontsize = 18)\n",
    "\n",
    "ax3 = fig2.add_subplot(1,2,1)\n",
    "ax3.plot((vect1),(vect2), color = '#1f77b4')\n",
    "ax3.set_xlabel(r\"$m_{A'}$ [GeV]\", fontsize = 14)\n",
    "ax3.set_ylabel(r\"Branching Ratio: $A' \\rightarrow e^+ + e^-$\", fontsize = 14)\n",
    "ax3.set_title('num_Signal = {0} Resolution'.format(num_Signal), fontsize = 14, loc = 'right')\n",
    "plt.grid()\n",
    "\n",
    "ax2 = fig2.add_subplot(1,2,2)\n",
    "ax2.plot((xListBranch), (yListBranch), color = '#1f77b4')\n",
    "ax2.set_xlabel(r\"$m_{A'}$ [GeV]\", fontsize = 14)\n",
    "ax2.set_ylabel(r\"Branching Ratio: $A' \\rightarrow e^+ + e^-$\", fontsize = 14)\n",
    "ax2.set_title('num_Branch = {0} Resolution'.format(num_Branch), fontsize = 14, loc = 'right')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T20:51:09.799117Z",
     "start_time": "2018-03-22T20:51:09.787988Z"
    }
   },
   "source": [
    "### Initialize Signal Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell:\n",
    "\n",
    "1. Initializes the signal data frame\n",
    "\n",
    "2. Appends `num_Signal` $\\varepsilon$ and $m_{A'}$ points. By default, the range for mediator mass and kinetic mixing are dictated by the range of $m_{A'}$ in the Sommerfeld file, and $10^{-11} \\leq \\varepsilon \\leq 10^{-5}$. To change the range of $\\varepsilon$, adjust the arguments of the variable `epsilonLogRange`.\n",
    "\n",
    "3. Appends the column headers to the data frame\n",
    "\n",
    "The columns are as follows. If the column values have units, we have included units in square brackets in the column headers.\n",
    "\n",
    "- `counter`: Explicitly stores the current row of the file. This is used as a loop variable to resume the calculation should it be stopped.\n",
    "\n",
    "- `mX[GeV]`: Stores the value of $m_X$ taken directly from the $\\texttt{Sommerfeld.csv}$ file\n",
    "\n",
    "- `mA[GeV]`: Stores `num_Signal` values of $m_A$ in the range $0.01 \\ \\text{GeV} \\leq m_{A'} \\leq 10 \\ \\text{GeV}$\n",
    "\n",
    "- `Epsilon`: Stores `num_Signal` values of $\\varepsilon$ in the range $10^{-11} \\leq \\varepsilon \\leq 10^{-5}$\n",
    "\n",
    "- `Capture[s-1]`: Stores the value of $C_\\text{cap}$ for the values in the corresponding row\n",
    "\n",
    "- `Annihilation[s-1]`: Stores the value of $C_\\text{ann}$ for the values in the corresponding row\n",
    "\n",
    "- `GammaAnn[s-1]`: Stores the value of $\\Gamma_\\text{ann}$ for the values in the corresponding row\n",
    "\n",
    "- `IceCubeSignal`: Stores the value of $N_{sig}$ for the values in the corresponding row\n",
    "\n",
    "- `LiveTime[s]`: Stores the live time of the experiment in seconds. By default, this is 10 years.\n",
    "\n",
    "- `SommerfeldFile`: Stores the name of the working Sommerfeld file\n",
    "\n",
    "- `BranchingRatioFile`: Stores the name of the working branching ratio file\n",
    "\n",
    "- `Resolution`: Stores the value for the parameter `num_Signal`\n",
    "\n",
    "- `Kappa0[GeV5]`: Stores the value of $\\kappa_0$ taken directly from the $\\texttt{Sommerfeld.csv}$ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:33:24.259054Z",
     "start_time": "2019-06-19T20:33:24.212033Z"
    }
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Define parameter space Ranges\n",
    "#########################################\n",
    "# Logarithmic Spacing because we plot this in log-log space\n",
    "# 0.01 GeV < mA < 10 GeV\n",
    "# 10^-11 < Epsilon < 10^-5\n",
    "\n",
    "ma_Min = (min(mAData))\n",
    "ma_Max = (max(mAData))\n",
    "\n",
    "epsilon_Min = 10**-11\n",
    "epsilon_Max = 10**-5\n",
    "\n",
    "# Use Logarithmic spacing since we plot the log of everything\n",
    "mALogRange = np.logspace(np.log10(ma_Min), np.log10(ma_Max), num_Signal, base = 10)\n",
    "epsilonLogRange = np.logspace(np.log10(epsilon_Min), np.log10(epsilon_Max), num_Signal, base = 10)\n",
    "\n",
    "#########################################\n",
    "# Initialize Master Arrays\n",
    "#########################################\n",
    "masterSignalArray = []\n",
    "\n",
    "#########################################\n",
    "# Populate the Master Arrays with the column headers and 'None' Cells\n",
    "#########################################\n",
    "counter = 0\n",
    "for mATemp in mALogRange:\n",
    "    for epsilonTemp in epsilonLogRange:\n",
    "        masterSignalArray.append([counter, ' ', mATemp, epsilonTemp, \\\n",
    "                                  'None', 'None', 'None', 'None', ' ', \\\n",
    "                                  ' ', ' ', ' ', ' '])\n",
    "        counter +=1\n",
    "#########################################\n",
    "# Create a PANDAS DataFrame from the Master Arrays\n",
    "#########################################\n",
    "masterSignalDataframe = pd.DataFrame(data = masterSignalArray, \\\n",
    "    columns=['Counter', 'mX[GeV]', 'mA[GeV]', 'Epsilon',\\\n",
    "             'Capture[s-1]', 'Annihilation[s-1]', 'GammaAnn[s-1]', 'IceCubeSignal', 'LiveTime[s]', \\\n",
    "             'SommerfeldFile', 'BranchingRatioFile','Resolution','Kappa0[GeV5]'])\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to External .csv File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T20:57:01.532084Z",
     "start_time": "2018-03-22T20:57:01.522605Z"
    }
   },
   "source": [
    "This cell writes the signal data frame to an external csv file. The naming convention should follow the convention outlined in the [Create Sommerfeld.csv](#Create-Sommerfeld.csv) section, namely:\n",
    "    \n",
    "    <#><unit>Signal.csv\n",
    "\n",
    "So for $m_X = 100$ GeV, the corresponding signal file is named:\n",
    "\n",
    "    100GeVSignal.csv\n",
    "\n",
    "This is the final step before starting the time-consuming calcuation process to populate $\\texttt{Signal.csv}$ with data. As a final check, we prompt the user for input to ensure that all model and experimental parameters are correct before starting the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:33:39.902952Z",
     "start_time": "2019-06-19T20:33:27.496563Z"
    }
   },
   "outputs": [],
   "source": [
    "signalFileName = input('Signal Rate filename: ')\n",
    "if signalFileName == '':\n",
    "    print (\"Using default filename\")\n",
    "    signalFileName = 'Signal_DEFAULT.csv'\n",
    "signalFile = signalPath(signalFileName)\n",
    "assert (signalFileName[-4:] == '.csv'), 'Must inclue \".csv\" in the file name'\n",
    "\n",
    "#############################\n",
    "# Double check that these values are correct\n",
    "#############################\n",
    "liveTime = DarkCapPy.Configure.Conversions.yr2s(10)\n",
    "\n",
    "print ('--------------------')\n",
    "print ('You are about to write the following to {0}'.format(signalFileName))\n",
    "print ('mX[GeV]               : {0}'.format(m_XSommerfeld))\n",
    "print ('mA_Low[GeV]           : {0}'.format(ma_Min))\n",
    "print ('mA_High[GeV]          : {0}'.format(ma_Max))\n",
    "print ('Epsilon_Low           : {0}'.format(epsilon_Min))\n",
    "print ('Epsilon_High          : {0}'.format(epsilon_Max))\n",
    "print ('Kappa0[GeV5]          : {0}'.format(kappa0Sommerfeld))\n",
    "print ('Observation Time[sec] : {0}'.format(liveTime))\n",
    "print ('Sommerfeld File       : {0}'.format(SommFileName))\n",
    "print ('Branching Ratio File  : {0}'.format(branchFileName))\n",
    "print ('Resolution            : {0}'.format(num_Signal))\n",
    "print ()\n",
    "\n",
    "yesNo = input ('Write to {0}.csv? (y/n) '.format(signalFileName))\n",
    "assert (yesNo == 'y' or yesNo == 'n'), 'Invalid input, must be \"y\" or \"n.\"'\n",
    "\n",
    "if ((yesNo == 'y') or (yesNo == 'Y')):\n",
    "    kappa0Value = 'None'\n",
    "    m_XValue = 'None'\n",
    "    \n",
    "    masterSignalDataframe.at[0, 'mX[GeV]']            = m_XSommerfeld\n",
    "    masterSignalDataframe.at[0, 'Kappa0[GeV5]']       = kappa0Sommerfeld\n",
    "    masterSignalDataframe.at[0, 'LiveTime[s]']        = liveTime\n",
    "    masterSignalDataframe.at[0, 'SommerfeldFile']     = SommFileName\n",
    "    masterSignalDataframe.at[0, 'BranchingRatioFile'] = branchFileName\n",
    "    masterSignalDataframe.at[0, 'Resolution']         = num_Signal\n",
    "\n",
    "    masterSignalDataframe.to_csv(signalFile, index=False)\n",
    "    \n",
    "    print ('Write executed')\n",
    "    \n",
    "else:\n",
    "    print ('Write aborted')\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal.csv Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** This cell is the main work-horse of this notebook**\n",
    "\n",
    "This is the cell where the $\\texttt{Signal.csv}$ file is populated with $N_\\text{sig}$ values by scanning over $m_{A'}$ and $\\varepsilon$. To begin or resume calculations for a $\\texttt{Signal.csv}$ file, this is the single cell to run. \n",
    "\n",
    "The algorithm is:\n",
    "1. Input an incomplete $\\texttt{Signal.csv}$ file\n",
    "2. Create a backup of that file. This ensures that even if the calculation is interrupted and the manual overwrite is not run, that not all data will be lost.\n",
    "3. Generate interpolations of the Sommerfeld effects and branching ratio \n",
    "4. Iterate through the `counter` column looking at the value of `IceCubeSignal`\n",
    "    4. If `IceCubeSignal` is `None`, compute all values in that row\n",
    "    4. Else, continue to loop\n",
    "5. Append newly calculated values to $\\texttt{Signal.csv}$\n",
    "6. Overwrite $\\texttt{Signal.csv}$\n",
    "7. Loop steps 2-6 until all rows are calculated\n",
    "\n",
    "\n",
    "Since this is the main working cell in this notebook, a number of features have been implimented for user-convenience.\n",
    "\n",
    "After user generates a $\\texttt{Signal.csv}$ file following the steps above, this cell calculates and populates a signal file. Depending on the resolution of the plot, this calculation may take $\\mathcal{O}(10 \\ \\text{hours})$ to complete. This cell saves each iteration to $\\texttt{Signal.csv}$ so that in case the Jupyter kernel is interrupted, the calculation may be resumed at another time. If the calculation is interrupted, one must run the cell titled Manual Overwrite to avoid data loss. \n",
    "\n",
    "To resume this calculation, simply run this cell and input the incomplete $\\texttt{Signal.csv}$ file to continue calculations. The algorithm will iterate through the `counter` column until it finds a row which has not been calculated.\n",
    "\n",
    "As a benchmark, with a resolution of `num_Signal = 200`, the upper left plot of Fig. 3 in [arXiv:1509.07525](https://arxiv.org/abs/1509.07525) took about 8 hours of run time on a modern laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:34:16.051286Z",
     "start_time": "2019-06-19T20:34:10.125088Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Read in Signal File\n",
    "#########################################\n",
    "signalFileName = input('Signal Rate File: ')\n",
    "signalFile = signalPath(signalFileName)\n",
    "signalDataIn = pd.read_csv(signalFile, sep = ',')\n",
    "\n",
    "#########################################\n",
    "# Create backup of Signal File \n",
    "#########################################\n",
    "now = datetime.now()\n",
    "timeStamp = now.strftime('%b-%d_%H.%M')\n",
    "signalFileNameCopy = signalFileName[:-4] + '_' + timeStamp + '.csv'\n",
    "signalDataIn.to_csv(signalBackupPath(signalFileNameCopy), sep=',', index=False)\n",
    "print ('Backup Created')\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Constants in (mA, epsilon) space:\n",
    "#########################################\n",
    "signalm_X = float(signalDataIn.at[0, 'mX[GeV]'])\n",
    "FileKappa0 = float(signalDataIn.at[0, 'Kappa0[GeV5]'])\n",
    "Alpha = 1./137\n",
    "LiveTime = float(signalDataIn.at[0, 'LiveTime[s]'])\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Sommerfeld Interpolation\n",
    "#########################################\n",
    "sommFileName = signalDataIn.at[0,'SommerfeldFile']\n",
    "sommDataIn = pd.read_csv(sommerfeldPath(sommFileName))\n",
    "mAData = sommDataIn['mA[GeV]']\n",
    "SommDataValues = sommDataIn['ThermAvgSommerfeld']\n",
    "SommInterp = interpolate.interp1d(mAData,SommDataValues)\n",
    "\n",
    "#########################################\n",
    "# Branching Ratio Interpolation\n",
    "#########################################\n",
    "branchFileName = signalDataIn.at[0,'BranchingRatioFile']\n",
    "branchDataIn = pd.read_csv(branchPath(branchFileName))\n",
    "mABranch = branchDataIn['mA[GeV]']\n",
    "branchingDataValues = branchDataIn['BR']\n",
    "branchRatioInterp = interpolate.interp1d(mABranch,branchingDataValues)\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Begin iterating through the file\n",
    "#########################################\n",
    "loopRange = range(0,len(signalDataIn['Counter']))\n",
    "print ()\n",
    "print ('--------------------')\n",
    "finishedCounter = 0\n",
    "index = 0\n",
    "for index in loopRange:\n",
    "    testValue = signalDataIn.loc[index, 'IceCubeSignal']\n",
    "    \n",
    "    if (testValue == 'None'):\n",
    "        #########################################\n",
    "        # Initialize iteration-specific Quantities\n",
    "        #########################################\n",
    "        signalm_ATemp = float(signalDataIn.at[index, 'mA[GeV]'])\n",
    "        FileEpsilonTemp = float(signalDataIn.at[index, 'Epsilon'])\n",
    "        Alpha_X = DP.alphaTherm(signalm_X, signalm_ATemp)      \n",
    "#         Sommerfeld = SommInterp(signalm_ATemp)\n",
    "        Sommerfeld = 1\n",
    "        BranchingRatio = branchRatioInterp(signalm_ATemp)\n",
    "        \n",
    "        #########################################\n",
    "        # Parameter Spce Calculations\n",
    "        #########################################\n",
    "        Capture = DP.cCapQuick(signalm_X, signalm_ATemp, FileEpsilonTemp, Alpha_X, FileKappa0)\n",
    "        SigmaVTree = DP.sigmaVtree(signalm_X, signalm_ATemp, Alpha_X)\n",
    "        Annihilation = DP.cAnn(signalm_X, SigmaVTree, thermAvgSomm = Sommerfeld)\n",
    "        GammaAnn = DP.gammaAnn(Capture, Annihilation)\n",
    "        L = DP.decayLength(signalm_X, signalm_ATemp, FileEpsilonTemp, BranchingRatio)\n",
    "        EpsilonDecay = DP.epsilonDecay(L)\n",
    "        \n",
    "        IceCubeSignal = DP.iceCubeSignal(GammaAnn, EpsilonDecay, LiveTime)\n",
    "\n",
    "        #########################################\n",
    "        # Append Calculated Values to SignalFile.csv\n",
    "        #########################################\n",
    "        signalDataIn.at[index, 'Capture[s-1]']      = Capture\n",
    "        signalDataIn.at[index, 'Annihilation[s-1]'] = Annihilation\n",
    "        signalDataIn.at[index, 'GammaAnn[s-1]']     = GammaAnn\n",
    "        signalDataIn.at[index, 'IceCubeSignal']     = IceCubeSignal\n",
    "        \n",
    "        \n",
    "        #########################################\n",
    "        # Overwrite SignalFile.csv\n",
    "        #########################################\n",
    "        signalDataIn.to_csv(signalFile, sep=',',index=False) \n",
    "        finishedCounter += 1\n",
    "        \n",
    "        if (index % 20 == 0):\n",
    "            print ('Calculations for index: {0} recorded'.format(index))\n",
    "        \n",
    "    elif (testValue != 'None'):\n",
    "        finishedCounter += 1\n",
    "    \n",
    "    if (finishedCounter == len(signalDataIn['Counter'])):\n",
    "        print ('--------------------')\n",
    "        print ('All Calculations Complete')\n",
    "        break\n",
    "        \n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is the manual overwrite of the $\\texttt{Signal.csv}$ file.\n",
    "\n",
    "If the previous cell has been interrupted, this cell must be run to write the data stored in Jupyter memory into $\\texttt{Signal.csv}$. This is because if the Jupyter kernel happens to be interrupted during the `to_csv` function, there is a chance the entire $\\texttt{Signal.csv}$ file will be blank. The manual overwrite cell writes the dataframe stored in Jupyter memory to $\\texttt{Signal.csv}$ to ensure that no data will be lost in the interruption process. Failure to run the manual overwrite cell after interruption may result in a loss of the data stored in $\\texttt{Signal.csv}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:34:20.733141Z",
     "start_time": "2019-06-19T20:34:19.013859Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print ('Working Signal File: {0}'.format(signalFileName))\n",
    "\n",
    "overwrite = input('Overwrite? (y/n): ')\n",
    "if ((overwrite == 'y') or (overwrite == 'Y')):\n",
    "    signalDataIn.to_csv(signalFile, sep=',',index=False) \n",
    "    print ('Overwrite complete')\n",
    "    \n",
    "else:\n",
    "    print ('Overwrite aborted')\n",
    "\n",
    "print ('Complete')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T00:56:56.735737Z",
     "start_time": "2018-08-08T00:56:56.724766Z"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Signal Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell reads in a completed $\\texttt{Signal.csv}$ and plots the mediator mass $m_{A'}$ on the horizontal axis, the kinetic mixing parameter $\\varepsilon$ on the vertical axis, and the signal event contours $N_\\text{sig} = \\{ 1, 10, 100, 1000 \\}$ at IceCube.\n",
    "\n",
    "If an incomplete $\\texttt{Signal.csv}$ file is read into this cell, Python will throw:\n",
    "```python\n",
    "AttributeError: 'str' object has no attribute 'log10'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:34:28.398495Z",
     "start_time": "2019-06-19T20:34:22.611620Z"
    }
   },
   "outputs": [],
   "source": [
    "signalFile = input('Signal Rate filename: ')\n",
    "readFile = signalPath(signalFile)\n",
    "rawData = pd.read_csv(readFile)\n",
    "Filem_X = float(rawData.loc[0,'mX[GeV]'])\n",
    "\n",
    "#Pull out min / max values on each axis\n",
    "xMin = min(rawData['mA[GeV]'])\n",
    "xMax = max(rawData['mA[GeV]'])\n",
    "yMin = min(rawData['Epsilon'])\n",
    "yMax = max(rawData['Epsilon'])\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Define the dimension of the grid. \n",
    "# For an n x n grid, this will pull out the number \"n\"\n",
    "#########################################\n",
    "gridDim = int(rawData.loc[0,'Resolution'])\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Create the n x n grid\n",
    "#########################################\n",
    "# Logarithmic Spacing\n",
    "xLogRange = np.logspace(np.log10(xMin), np.log10(xMax), gridDim, base = 10)\n",
    "yLogRange = np.logspace(np.log10(yMin), np.log10(yMax), gridDim, base = 10)\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Read in and reshape the Contour data\n",
    "#########################################\n",
    "# This reads in contour data as a single column,\n",
    "#     but we reshape it to a 2D, n x n array to fit the grid\n",
    "\n",
    "z = [rawData['IceCubeSignal']]\n",
    "SignalData = np.reshape(z, (len(xLogRange), len(yLogRange)))\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "# Set the correct units for m_X\n",
    "###############################\n",
    "m_XUnit = 'None'\n",
    "if (Filem_X < 1000):\n",
    "    m_XDisplay = Filem_X\n",
    "    m_XUnit = 'GeV'\n",
    "\n",
    "if (Filem_X >= 1000):\n",
    "    m_XDisplay = Filem_X * 10**-3\n",
    "    m_XUnit = 'TeV'\n",
    "    \n",
    "    \n",
    "###############################\n",
    "# Plotting\n",
    "###############################\n",
    "signalFig = plt.figure(figsize = (6,6))\n",
    "\n",
    "CP = plt.contour(np.log10(xLogRange), np.log10(yLogRange), \\\n",
    "                 np.log10(SignalData.transpose()),\\\n",
    "                 levels=[0,1,2,3],\\\n",
    "                )\n",
    "\n",
    "xLabelNums = [-2,-1,0,1]\n",
    "xLabelText = [r'$10^{-2}$',r'$10^{-1}$',r'$10^{0}$',r'$10^{1}$']\n",
    "\n",
    "yLabelNums = [-5,-6,-7,-8,-9,-10,-11]\n",
    "yLabelText = [r'$10^{-5}$', r'$10^{-6}$', r'$10^{-7}$', r'$10^{-8}$', \\\n",
    "              r'$10^{-9}$', r'$10^{-10}$', r'$10^{-11}$']\n",
    "\n",
    "\n",
    "plt.xticks(xLabelNums, xLabelText, fontsize = 12)\n",
    "plt.yticks(yLabelNums, yLabelText, fontsize = 12)\n",
    "\n",
    "plt.xlabel(r\"$m_{A'}$ $[GeV]$\", fontsize = 14)\n",
    "plt.ylabel(r'$\\varepsilon$', fontsize = 18)\n",
    "plt.suptitle('IceCube Events', fontsize = 16)\n",
    "plt.title(r'$m_X$ = {0} {1}'.format(m_XDisplay, m_XUnit), loc = 'right', fontsize = 13)\n",
    "\n",
    "labels = [r'1', r'$10$',r'$10^2$',r'$10^3$']\n",
    "#for i in range(len(labels)):\n",
    "#    CP.collections[i].set_label(labels[i])\n",
    "\n",
    "plt.legend(loc='upper right', fontsize = 13)\n",
    "plt.grid()\n",
    "\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SignalData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:05:50.733025Z",
     "start_time": "2018-08-09T18:05:43.643880Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figName = input('Filename: ')\n",
    "\n",
    "signalFig.savefig((figName), dpi = 700,\n",
    "             bbox_inches = 'tight', # If this isn't here, it partially cuts off the axis labels\n",
    "            )\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
